{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synonym_dict\n",
    "synonym_dict = {\n",
    "    \"đôi\": \"cặp\",\n",
    "    \"couple\": \"cặp\",\n",
    "    \"mũ\": \"nón\",\n",
    "    \"gile\": \"ghile\",\n",
    "    \"ghi lê\": \"ghile\",\n",
    "    \"triệt\": \"nhổ\",\n",
    "    \"tựa\": \"dựa\",\n",
    "    \"nến\": \"đèn cầy\",\n",
    "    \"bó\": \"ép\",\n",
    "    \"dây nịt\": \"thắt lưng\",\n",
    "    \"gôn\": \"golf\",\n",
    "    \"sang\": \"to\",\n",
    "    \"adapter\": \"cáp chuyển\",\n",
    "    \"bao tay\": \"găng tay\",\n",
    "    \"khử\": \"trừ\",\n",
    "    \"khăn choàng\": \"khăn quàng\",\n",
    "    \"cốc\": \"ly\",\n",
    "    \"thái thịt\": \"cắt thịt\",\n",
    "    \"bào đá\": \"xay đá\",\n",
    "    \"áo lưới\": \"áo ren\",\n",
    "    \"xà phòng\": \"xà bông\",\n",
    "    \"bạt phủ\": \"áo phủ\",\n",
    "    \"áo trùm\": \"áo phủ\",\n",
    "    \"bạt trùm\": \"áo phủ\",\n",
    "    \"splitter\": \"bộ chia\",\n",
    "    \"nắp lưng\": \"vỏ lưng\",\n",
    "    \"phích cắm\": \"ổ cắm\",\n",
    "    \"cafe\": \"cà phê\",\n",
    "    \"ôtô\": \"xe hơi\",\n",
    "    \"ô tô\": \"xe hơi\",\n",
    "    \"oto\": \"xe hơi\",\n",
    "    \"drap\": \"ga\",\n",
    "    \"case\": \"ốp lưng\",\n",
    "    \"balo\": \"túi xách\",\n",
    "    \"ba lô\": \"túi xách\",\n",
    "    \"balô\": \"túi xách\",\n",
    "    \"cặp xách\": \"túi xách\",\n",
    "    \"headphone\": \"tai nghe\",\n",
    "    \"gold\": \"vàng\",\n",
    "    \"xoè\": \"xòe\",\n",
    "    \"massage\": \"mát xa\",\n",
    "    \"tay ngắn\": \"ngắn tay\",\n",
    "    \"tay dài\": \"dài tay\",\n",
    "    \"váy\": \"đầm\",\n",
    "    \"sạc nhanh\": \"fast charge\",\n",
    "    \"bóp\": \"ví\",\n",
    "    \"áo ngực\": \"áo lót\",\n",
    "    \"hdd cable\": \"cáp ổ cứng\",\n",
    "    \"makbook\": \"macbook\",\n",
    "    \"mabook\": \"macbook\",\n",
    "    \"fan\": \"quạt\",\n",
    "    \"chè\": \"trà\",\n",
    "#     \"ô\": \"dù\",\n",
    "    \"masage\": \"mát xa\",\n",
    "    \" i \": \" \",\n",
    "    \"6 plus\": \"6+\",\n",
    "    \"7 plus\": \"7+\",\n",
    "    \" pro\": \"_pro\",\n",
    "    \"iphone 6\": \"iphone_6\",\n",
    "    \"iphone 6 plus\": \"iphone_6+\",\n",
    "    \"iphone 7\": \"iphone_7\",\n",
    "    \"iphone 7 plus\": \"iphone_7+\",\n",
    "    \"iphone 7plus\": \"iphone_7+\",\n",
    "    \"iphone x\": \"iphone 10\",\n",
    "    \" ii\": \" 2\",\n",
    "    \"thái\": \"cắt\",\n",
    "    \"sạc\": \"nạp\",\n",
    "    \"i pad\": \"ipad\"\n",
    "}\n",
    "\n",
    "special_character = ['|', ',', ':', '\\'', '\"', '(', ')', '{', '}', '[',']', '_', '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary functions\n",
    "def remove_special_character(sentence):\n",
    "    for c in special_character:\n",
    "        sentence = sentence.replace(c,\" \")\n",
    "    return sentence\n",
    "\n",
    "def replace_synonym(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    for k,v in synonym_dict.items():\n",
    "        sentence = sentence.replace(k,v)\n",
    "    return sentence\n",
    "\n",
    "def check_duplicate(word_list,n):\n",
    "    ngrams = list(nltk.ngrams(word_list,n))\n",
    "    freq = []\n",
    "    for ngram in set(ngrams):\n",
    "        sentence = \" \".join(word_list)\n",
    "        substring = \" \".join(ngram)\n",
    "        count = sentence.count(substring)\n",
    "        freq.append((ngram,count))\n",
    "        \n",
    "    freq = sorted(freq, key = lambda x: x[1])\n",
    "    return freq\n",
    "\n",
    "def max_ngrams(sentence):\n",
    "    # create a word list\n",
    "    word_list = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # binary search for max ngrams\n",
    "    max_len = int(len(word_list)/2)\n",
    "    min_len = 1\n",
    "    \n",
    "    result_word = None\n",
    "    while 1:\n",
    "        current_len = int((min_len + max_len)/2)\n",
    "        [my_word, num_appear] = check_duplicate(word_list, current_len).pop()\n",
    "        if num_appear > 1: # has repeated\n",
    "            [result_word, result_appear] = [my_word, num_appear]\n",
    "            min_len = current_len + 1\n",
    "            if min_len > max_len:\n",
    "                break\n",
    "        else: # no repeat\n",
    "            max_len = current_len - 1\n",
    "            if max_len < min_len:\n",
    "                break\n",
    "    \n",
    "    [result_word_1, result_appear_1] = check_duplicate(word_list,min_len).pop()\n",
    "    if result_word == None:\n",
    "        return [result_word_1,result_appear_1]\n",
    "    else:\n",
    "        if result_appear_1 > result_appear:\n",
    "            return [result_word_1,result_appear_1]\n",
    "        else:\n",
    "            return [result_word,result_appear]\n",
    "    \n",
    "def remove_repeat_word(sentence):\n",
    "    word_list = nltk.word_tokenize(sentence)\n",
    "    my_dict = {}\n",
    "    original_len = len(word_list)\n",
    "    for word in word_list:\n",
    "        if my_dict.get(word) == None:\n",
    "            my_dict[word] = 1\n",
    "        else:\n",
    "            my_dict[word] += 1\n",
    "            \n",
    "    count = 0\n",
    "    for word in word_list:\n",
    "        if my_dict[word] == 1:\n",
    "            count += 1\n",
    "            \n",
    "    ratio = count/original_len\n",
    "    return [original_len, count, ratio]\n",
    "\n",
    "def min_word_distance(sentence):\n",
    "    [result_word, result_appear] = max_ngrams(sentence)\n",
    "    substring = \" \".join(result_word)\n",
    "    \n",
    "    # sentence without substring\n",
    "    if sentence.count(substring) < 2:\n",
    "        return (len(list(nltk.word_tokenize(sentence))),1,0,0)\n",
    "    \n",
    "    my_sentence = sentence.replace(substring,'')\n",
    "    len_word = len(list(nltk.word_tokenize(my_sentence)))\n",
    "    #len_word = len(list(nltk.word_tokenize(sentence)))\n",
    "        \n",
    "    substring_list = sentence.split(substring)\n",
    "    \n",
    "    if substring_list[0] == '':\n",
    "        isBegin = 1\n",
    "    else:\n",
    "        isBegin = 0\n",
    "        \n",
    "    if substring_list[-1] == '':\n",
    "        isEnd = 1\n",
    "    else:\n",
    "        isEnd = 0\n",
    "        \n",
    "    if (len(substring_list[0]) == 0) and (len(substring_list[-1]) == 0):\n",
    "        return (0,0,isBegin,isEnd)\n",
    "    \n",
    "    result = []\n",
    "    for i,value in enumerate(substring_list):\n",
    "        tmp = list(nltk.word_tokenize(value))\n",
    "        if (i == 0) or (i == len(substring_list)-1):\n",
    "            pass\n",
    "        else:\n",
    "            result.append((value,len(tmp)))\n",
    "            \n",
    "    result = sorted(result, key = lambda x: x[1])\n",
    "    \n",
    "    if len_word == 0:\n",
    "        len_word = 1;\n",
    "        \n",
    "    return (result[0][1],result[0][1]/len_word,isBegin,isEnd)\n",
    "\n",
    "def convert_to_non_diacritics(title):\n",
    "    my_dict = {\n",
    "        # character a\n",
    "        'à': 'a', 'ả': 'a', 'ã': 'a', 'á': 'a', 'ạ': 'a',\n",
    "        'ă': 'a', 'ằ': 'a', 'ẳ': 'a', 'ẵ': 'a', 'ắ': 'a',\n",
    "        'ặ': 'a', 'â': 'a', 'ầ': 'a', 'ẩ': 'a', 'ẫ': 'a',\n",
    "        'ấ': 'a', 'ậ': 'a',\n",
    "        \n",
    "        # character e\n",
    "        'è': 'e', 'ẻ': 'e', 'ẽ': 'e', 'é': 'e', 'ẹ': 'e',\n",
    "        'ê': 'e', 'ề': 'e', 'ể': 'e', 'ễ': 'e', 'ế': 'e',\n",
    "        'ệ': 'e',\n",
    "        \n",
    "        # character i\n",
    "        'ì': 'i', 'ỉ': 'i', 'ĩ': 'i', 'í': 'i', 'ị': 'i',\n",
    "        \n",
    "        # character o\n",
    "        'ò': 'o', 'ỏ': 'o', 'õ': 'o', 'ó': 'o', 'ọ': 'o',\n",
    "        'ơ': 'o', 'ờ': 'o', 'ở': 'o', 'ỡ': 'o', 'ớ': 'o',\n",
    "        'ợ': 'o', 'ô': 'o', 'ồ': 'o', 'ổ': 'o', 'ỗ': 'o',\n",
    "        'ố': 'o', 'ộ': 'o',\n",
    "        \n",
    "        # character u\n",
    "        'ù': 'u', 'ủ': 'u', 'ũ': 'u', 'ú': 'u', 'ụ': 'u',\n",
    "        'ư': 'u', 'ừ': 'u', 'ử': 'u', 'ữ': 'u', 'ứ': 'u',\n",
    "        'ự': 'u',\n",
    "        \n",
    "        # character y\n",
    "        'ỳ': 'y', 'ỷ': 'y', 'ỹ': 'y', 'ý': 'y', 'ỵ': 'y',\n",
    "        \n",
    "        # character d\n",
    "        'đ': 'd'\n",
    "    }\n",
    "    \n",
    "    s = \"\"\n",
    "    for c in title:\n",
    "        if my_dict.get(c) != None:\n",
    "            s += my_dict[c]\n",
    "        else:\n",
    "            s += c\n",
    "    return s\n",
    "\n",
    "def choose_title(title):\n",
    "    title = replace_synonym(title)\n",
    "    title_non_diacritics = convert_to_non_diacritics(title)\n",
    "    if ((len(max_ngrams(title)[0]) + 2 <= len(max_ngrams(title_non_diacritics)[0])) or\n",
    "        ((len(max_ngrams(title_non_diacritics)[0]) >= 3) \n",
    "             and (len(max_ngrams(title_non_diacritics)[0]) > len(max_ngrams(title)[0])))):\n",
    "        return title_non_diacritics\n",
    "    return title\n",
    "\n",
    "def is_valid(s1,s2):\n",
    "    word_list1 = s1.split(\" \")\n",
    "    word_list2 = s2.split(\" \")\n",
    "    \n",
    "    if (s2.find(s1) != -1) or (s1.find(s2) != -1):\n",
    "        return False\n",
    "    \n",
    "    if len(word_list1) <= len(word_list2):\n",
    "        list1 = word_list1.copy()\n",
    "        list2 = word_list2.copy()\n",
    "    else:\n",
    "        list1 = word_list2.copy()\n",
    "        list2 = word_list1.copy()\n",
    "        \n",
    "    curr_idx = 0\n",
    "    count = 0\n",
    "    for word1 in list1:\n",
    "        flag = True\n",
    "        for i,word2 in enumerate(list2[curr_idx:],start = curr_idx):\n",
    "            if word1 == word2:\n",
    "                curr_idx = i + 1\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def my_tokenize(s,n):\n",
    "    word_list = list(s.split(\" \"))\n",
    "    result = []\n",
    "    for i in range(len(word_list) - n + 1):\n",
    "        result.append(\" \".join(word_list[i:i+n]))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def check_skipgram(s,n):\n",
    "    sentence_list = my_tokenize(s,n)\n",
    "    curr_idx = 0\n",
    "    for sentence in sentence_list:        \n",
    "        idx = s[curr_idx:].find(sentence)\n",
    "        part1 = s[:max(0,curr_idx+idx-1)]\n",
    "        part2 = s[curr_idx+idx+len(sentence)+1:]\n",
    "        part1_tokenize = my_tokenize(part1,n+1)\n",
    "        part2_tokenize = my_tokenize(part2,n+1)\n",
    "        for token in part1_tokenize:\n",
    "            if is_valid(sentence,token):\n",
    "                return [sentence,token]\n",
    "        for token in part2_tokenize:\n",
    "            if is_valid(sentence,token):\n",
    "                return [sentence,token]\n",
    "            \n",
    "        curr_idx += idx\n",
    "\n",
    "    return [None,None]\n",
    "\n",
    "def check_skipgram_overall(s):\n",
    "    word_list = s.split(\" \")\n",
    "    n = int(len(word_list)/2)\n",
    "    tmp = [None,None]\n",
    "    while n >= 4:\n",
    "        tmp = check_skipgram(s,n)\n",
    "        if tmp[0] != None:\n",
    "            break\n",
    "        else:\n",
    "            n -= 1\n",
    "            \n",
    "    len_original_s = len(max_ngrams(s)[0])\n",
    "    if tmp == [None,None]:\n",
    "        len_skip_s = 0\n",
    "#         return [None,None,0]\n",
    "    else:\n",
    "        len_skip_s = len(list(tmp[1].split(\" \")))\n",
    "#         return [tmp[0],tmp[1],len(list(tmp[1].split(\" \")))]\n",
    "\n",
    "    if len_skip_s >= max(len_original_s,4):\n",
    "        return s.replace(tmp[0],tmp[1])\n",
    "    else:\n",
    "        return s\n",
    "    \n",
    "def preprocess(s):\n",
    "    s = remove_special_character(s)\n",
    "    s = s.lower()\n",
    "    \n",
    "    # remove multiple space\n",
    "    result = ''\n",
    "    isStart = 1\n",
    "    isSpace = 0\n",
    "    for c in s:\n",
    "        if c != ' ':\n",
    "            result += c\n",
    "            isSpace = 0\n",
    "        else: # c == ' '\n",
    "            if isStart == 1:\n",
    "                isStart = 0\n",
    "                pass\n",
    "            else:\n",
    "                if isSpace == 0:\n",
    "                    result += c\n",
    "                    isSpace = 1\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "    if result[-1] == ' ':\n",
    "        return result[:-1]\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features for title\n",
    "def create_feature(df_original_title):\n",
    "    # preprocess title\n",
    "    df_title = df_original_title.apply(lambda x: preprocess(x))\n",
    "    df_title = df_title.apply(lambda x: choose_title(x))\n",
    "    df_title = df_title.apply(lambda x: check_skipgram_overall(x))\n",
    "    \n",
    "    ngrams = df_title.apply(lambda x: max_ngrams(x))\n",
    "    non_repetition = df_title.apply(lambda x: remove_repeat_word(x))\n",
    "    min_distance = df_title.apply(lambda x: min_word_distance(x))\n",
    "    skipgram = df_title.apply(lambda x: check_skipgram_overall(x))\n",
    "    \n",
    "    # feature engineering step\n",
    "    feature_01 = ngrams.apply(lambda x: x[0])\n",
    "    feature_02 = ngrams.apply(lambda x: len(x[0]))\n",
    "    feature_03 = ngrams.apply(lambda x: x[1])\n",
    "    feature_04 = non_repetition.apply(lambda x: x[0])\n",
    "    feature_05 = non_repetition.apply(lambda x: x[1])\n",
    "    feature_06 = non_repetition.apply(lambda x: x[2])\n",
    "    feature_07 = min_distance.apply(lambda x: x[0])\n",
    "    feature_08 = min_distance.apply(lambda x: x[1])\n",
    "    feature_09 = min_distance.apply(lambda x: x[2])\n",
    "    feature_10 = min_distance.apply(lambda x: x[3])\n",
    "    \n",
    "    df_feature = pd.concat([\n",
    "        df_original_title,\n",
    "        feature_01,\n",
    "        feature_02,\n",
    "        feature_03,\n",
    "        feature_04,\n",
    "        feature_05,\n",
    "        feature_06,\n",
    "        feature_07,\n",
    "        feature_08,\n",
    "        feature_09,\n",
    "        feature_10\n",
    "    ], axis=1)\n",
    "    \n",
    "    df_feature.columns = ['title',\n",
    "                         'max_ngrams',\n",
    "                         'len_max_ngrams',\n",
    "                         'num_repetition_max_ngrams',\n",
    "                         'len_original_words',\n",
    "                         'len_non_repetition_words',\n",
    "                         'meaning_ratio',\n",
    "                         'min_distance',\n",
    "                         'relative_dist_ratio',\n",
    "                         'isBegin',\n",
    "                         'isEnd']\n",
    "    \n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn random forest model\n",
    "def learn_rf(train_X, train_y):\n",
    "    param_grid = [\n",
    "        {\n",
    "            'n_estimators': [200, 300, 400, 500, 1000, 2000],\n",
    "            'max_features': [2 ,3, 4, 5, 6, 7, 8, 9],\n",
    "            'bootstrap': [True, False],\n",
    "            'min_samples_split': [5, 10, 20, 50, 100]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    forest_classification = RandomForestClassifier()\n",
    "    \n",
    "    # train across 5 folds\n",
    "    rf_grid_search = GridSearchCV(forest_classification,\n",
    "                                 param_grid,\n",
    "                                 cv = 5,\n",
    "                                 n_jobs = -1)\n",
    "    \n",
    "    rf_grid_search.fit(train_X, train_y)\n",
    "    rf_best_model = rf_grid_search.best_estimator_\n",
    "    \n",
    "    return rf_best_model\n",
    "\n",
    "def calculate_prediction_prob_rf(rf_best_model,titles):\n",
    "    df_feature = create_feature(titles)\n",
    "    \n",
    "    # extract necessary data only\n",
    "    df_shortlisted = df_feature[['len_max_ngrams',\n",
    "                                'num_repetition_max_ngrams',\n",
    "                                'len_original_words',\n",
    "                                'len_non_repetition_words',\n",
    "                                'meaning_ratio',\n",
    "                                'min_distance',\n",
    "                                'relative_dist_ratio',\n",
    "                                'isBegin',\n",
    "                                'isEnd']]\n",
    "    \n",
    "    prediction_prob_rf = rf_best_model.predict_proba(df_shortlisted)[:,1]\n",
    "    \n",
    "    return prediction_prob_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train_X, train_y\n",
    "df = pd.read_csv('labeled_data_05.csv', header = None, encoding = 'utf-8')\n",
    "df.columns = ['id', 'title', 'class']\n",
    "df_feature = create_feature(df['title'])\n",
    "\n",
    "train_X = df_feature[['len_max_ngrams',\n",
    "                     'num_repetition_max_ngrams',\n",
    "                     'len_original_words',\n",
    "                     'len_non_repetition_words',\n",
    "                     'meaning_ratio',\n",
    "                     'min_distance',\n",
    "                     'relative_dist_ratio',\n",
    "                     'isBegin',\n",
    "                     'isEnd']]\n",
    "\n",
    "train_y = df['class']\n",
    "\n",
    "rf_best_model = learn_rf(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_b_0_search_result = pd.read_csv('b_0_search_result.txt', sep = '\\n', header = None, names = ['titles'])\n",
    "df_default_search_result = pd.read_csv('default_search_result.txt', sep = '\\n', header = None, names = ['titles'])\n",
    "\n",
    "# calculate prediction prob for titles\n",
    "predict_prob_b_0 = calculate_prediction_prob_rf(rf_best_model,df_b_0_search_result['titles'])\n",
    "predict_prob_default = calculate_prediction_prob_rf(rf_best_model,df_default_search_result['titles'])\n",
    "\n",
    "# output result\n",
    "b_0_search_result_predict_prob = pd.concat([df_b_0_search_result, pd.Series(predict_prob_b_0)], axis = 1)\n",
    "default_search_result_predict_prob = pd.concat([df_default_search_result, pd.Series(predict_prob_default)], axis = 1)\n",
    "\n",
    "# save file\n",
    "b_0_search_result_predict_prob.to_csv('b_0_predict_prob', sep='\\t', index = False)\n",
    "default_search_result_predict_prob.to_csv('default_predict_prob', sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
